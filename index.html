<!DOCTYPE HTML>
<html lang="en">
  <head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-3E7JJ239K6"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-3E7JJ239K6');
    </script>

    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yunshuang Li</title>

    <meta name="author" content="Jon Barron">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="images/head_robo.png" type="image/png">
    <!--<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
  </head>
  <style>
      .black-link {
      color: black; /* Set the desired color for the link */
    }
      .myself {
        color: black;
        text-decoration: underline;
      }
  </style>
  <style>
        body {
            font-family: Arial, sans-serif; /* Default font for the entire body */
        }

        .centered-text {
            text-align: center; /* Center-align text within the container */
            font-family: "Times New Roman", Times, serif; /* Different font for the centered text */
        }
  </style>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yunshuang Li
                </p>
                <p>I am a first-year Ph.D. student in Computer Science at University of Southern California, where I work with <a href="https://danielseita.github.io/">Prof. Daniel Seita</a> and <a href="https://uscresl.org/principal-investigator/">Prof. Gaurav Sukhatme</a>. I am interested in robot learning for general-purpose robot manipulation.
                </p>
                <p>
                  I earned a M.S. degree in Robotics at <a href="https://www.grasp.upenn.edu/">GRASP Lab</a>, University of Pennsyvania in May 2024. I was a member of Perception, Action, & Learnin (PAL) Research Group, advised by <a href="https://www.seas.upenn.edu/~dineshj/">Prof. Dinesh Jayaraman</a>. Previoulsy, I worked with <a href="https://www.cse.cuhk.edu.hk/~qdou/index.html">Prof. Qi Dou</a> at Chinese University of Hong Kong (CUHK) for an internship. I received my Honorable B.S. degree in Automation from <a href="http://ckc.zju.edu.cn/ckcen/">Chu Kochen Honors College</a>, Zhejiang University in 2022.
                </p>
                <p style="text-align:center">
                  <a href="mailto:yunshuan@usc.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/Yunshuang_Li_CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="data/Yunshuang-bio.txt">Bio</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=fpx2AWYAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/YunshuangL">Twitter</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/Yunshuang_headshot.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Yunshuang_headshot.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research interests lie in robot learning for general-purpose robot manipulation. During my PhD study, I would like to focus on learning for robot manipulation, exploring topics such as representation learning and policy learning.
                  My ultimate goal is to enable robots to manipulate and learn to manipulate like humans.
                </p>

                <p>
                  (* indicates equal contribution, ‚Ä† indicates equal advising)
                </p>
              </td>
            </tr>
          </tbody></table>
      
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           
            <tr onmouseout="UVD_stop()" onmouseover="UVD_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='UVD'>
                    <video  width=100% height=100% muted autoplay loop>
                    <source src="images/cloth_camera.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                    </video></div>
                  <img src='images/UVD.png' width="160">
                </div>
                <script type="text/javascript">
                  function UVD_start(){
                    document.getElementById('UVD').style.opacity = "1";
                  }
  
                  function UVD_stop() {
                    document.getElementById('UVD').style.opacity = "0";
                  }
                  UVD_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://zcczhang.github.io/UVD/">
                  <span class="papertitle">Universal Visual Decomposer: Long-Horizon Manipulation Made Easy</span>
                </a>
                <br>
                <a href="https://zcczhang.github.io/" class="black-link">Zichen Zhang</a>*,
                <a class="myself"><strong>Yunshuang Li</strong></a>*,
                <a href="https://obastani.github.io/" class="black-link">Osbert Bastani</a>,
                <a href="https://homes.cs.washington.edu/~abhgupta/" class="black-link">Abhishek Gupta</a>,
                <a href="https://www.seas.upenn.edu/~dineshj/" class="black-link">Dinesh Jayaraman</a>,
                <a href="https://www.seas.upenn.edu/~jasonyma/" class="black-link">Yecheng Jason Ma</a><sup>&dagger;</sup>,
                <a href="https://lucaweihs.github.io" class="black-link">Luca Weihs</a><sup>&dagger;</sup>
                <br>
                <em> <a href="https://leap-workshop.github.io/" class="black-link">Learning Effective Abstractions for Planning (LEAP)</a> worshop, CoRL <b>(oral, Best paper award)</b>, 2023</em> <br>
                <em> <a href="https://sites.google.com/view/fmdm-neurips23/" class="black-link">Foundation Models for Decision Making (FMDM)</a> worshop, NeurIPS <b>(oral, 6/112)</b>, 2023</em> <br>
                <em>International Conference on Robotics and Automation (ICRA), Best Computer Vision Paper Finalist, 2024</em>
                <br>
                <a href="https://arxiv.org/abs/2310.08581">arXiv</a>
                /
                <a href="https://zcczhang.github.io/UVD/">project page</a>
                /
                <a href=https://zcczhang.github.io/UVD/assets/videos/ICRA24_3197_VI_i.mp4>video</a>
                /
                <a href="https://github.com/zcczhang/UVD/">code</a>
                <p></p>
                <p>
                  We present Universal Visual Decomposer, an *off-the-shelf* task decomposition method that effectively produces semantically meaning subgoals across both simulated and real-robot environments for long-horizon visual manipulation tasks, without any task-specific knowledge or training. UVD discovered subgoals enable effective reward shaping for solving challenging multi-stage tasks using RL, and policies trained with IL exhibit significantly superior compositional generalization at test time.
                </p>
              </td>
            </tr>

            <tr onmouseout="im2contact_stop()" onmouseover="im2contact_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='im2contact'>
                  <video  width=100% height=100% muted autoplay loop>
                  <source src="images/im2contact.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                <img src='images/im2contact.png' width="160">
              </div>
              <script type="text/javascript">
                function im2contact_start(){
                  document.getElementById('im2contact').style.opacity = "1";
                }

                function im2contact_stop() {
                  document.getElementById('im2contact').style.opacity = "0";
                }
                im2contact_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=h8halpbqB-&referrer=%5Bthe%20profile%20of%20Dinesh%20Jayaraman%5D(%2Fprofile%3Fid%3D~Dinesh_Jayaraman2)">
                <span class="papertitle">Vision-Based Contact Localization Without Touch or Force Sensing</span>
              </a>
              <br>
              Leon Kim,
              <a class="myself"><strong>Yunshuang Li</strong></a>, 
              <a href="https://www.grasp.upenn.edu/people/michael-posa/" class="black-link">Michael Posa</a>,
              <a href="https://www.seas.upenn.edu/~dineshj/" class="black-link">Dinesh Jayaraman</a>
              <br>
              <em>Conference on Robot Learning (CoRL), 2023</em>
              <br>
              <a href="https://sites.google.com/view/im2contact/home">arXiv</a>
              /
              <a href="https://sites.google.com/view/im2contact/home">project page</a>
              /
              <a href="https://sites.google.com/view/im2contact/home">video</a>
              /
              <a href="https://sites.google.com/view/im2contact/home">code</a>
              <p></p>
              <p>
                We propose a challenging vision-based extrinsic contact localization task: with only a single RGB-D camera view of a robot workspace, identify when and where an object held by the robot contacts the rest of the environment. Our final approach im2contact demonstrates the promise of versatile general-purpose contact perception from vision alone, performing well for localizing various contact types (point, line, or planar; sticking, sliding, or rolling; single or multiple), and even under occlusions in its camera view.
              </p>
            </td>
          </tr>

            <tr onmouseout="MICCAI_stop()" onmouseover="MICCAI_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='MICCAI_image'>
                    <img src='images/challenge_after.png' width="160"></div>
                  <img src='images/challenge_before.png' width="160">
                </div>
                <script type="text/javascript">
                  function MICCAI_start() {
                    document.getElementById('MICCAI_image').style.opacity = "1";
                  }

                  function MICCAI_stop() {
                    document.getElementById('MICCAI_image').style.opacity = "0";
                  }
                  MICCAI_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0169260723002262">
                  <span class="papertitle">PEg TRAnsfer Workflow recognition challenge report: Do multimodal data improve recognition?</span>
                </a>
                <br>
                Arnaud Huaulm√©, Kanako Harada, (et al., including <a class="myself"><strong>Yunshuang Li</strong></a>, 
                <a href="https://www.researchgate.net/profile/Yonghao-Long-2" class="black-link">Yonghao Long</a>, 
                <a href="https://www.cse.cuhk.edu.hk/~qdou/" class="black-link">Qi Dou</a>)
                <br>
                <em>Computer Methods and Programs in Biomedicine, 2023</em>
                <br>
                <a href="https://arxiv.org/abs/2202.05821">arXiv</a>
                <p></p>
                <p>
                  This is the report paper on Workflow Recognition Challenge in MICCAI 2021. I lead the MedAIR team and rank the first over all the 5 rank method in one sub-challenge on multi-modal (videos and kinematics) workflow recognition of robotic surgery videos.
              </td>
            </tr>		


            <!-- <tr onmouseout="Collaborate_stop()" onmouseover="Collaborate_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='Collaborate_image'>
                    <img src='images/collaborative_1.png' width="160"></div>
                  <img src='images/collaborative_2.png' width="160">
                </div>
                <script type="text/javascript">
                  function Collaborate_start() {
                    document.getElementById('Collaborate_image').style.opacity = "1";
                  }

                  function Collaborate_stop() {
                    document.getElementById('Collaborate_image').style.opacity = "0";
                  }
                  Collaborate_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9517621">
                  <span class="papertitle">Collaborative Recognition of Feasible Region with Aerial and Ground Robots through DPCN</span>
                </a>
                <br>
                <a class="myself"><strong>Yunshuang Li</strong></a>,
                Zheyuan Huang, 
                <a href="https://www.researchgate.net/profile/Zexi-Chen-3" class="black-link">Zexi Chen</a>, 
                <a href="https://ywang-zju.github.io/" class="black-link">Yue Wang</a>, 
                <a href="https://www.researchgate.net/profile/Rong-Xiong" class="black-link">Rong Xiong</a>
                <br>
                <em>IEEE International Conference on Real-time Computing and Robotics (RCAR), 2021</em>
                <br>
                <a href="https://arxiv.org/abs/2103.00947v2">arXiv</a>
                <p></p>
                <p>
                  We present a collaborative system with aerial and ground robots to gain precise recognition of feasible region. Taking the aerial robots' advantages of having large scale variance of view points of the same route which the ground robots is on, the collaboration work provides global information of road segmentation for the ground robot, thus enabling it to obtain feasible region and adjust its pose ahead of time.
                </p>
              </td>
            </tr>		

            <tr onmouseout="SNN_stop()" onmouseover="SNN_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='SNN_image'>
                    <img src='images/SNN_after_large.png' width="160"></div>
                  <img src='images/SNN_before.png' width="160">
                </div>
                <script type="text/javascript">
                  function SNN_start() {
                    document.getElementById('SNN_image').style.opacity = "1";
                  }

                  function SNN_stop() {
                    document.getElementById('SNN_image').style.opacity = "0";
                  }
                  SNN_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://link.springer.com/chapter/10.1007/978-3-030-90525-5_79">
                  <span class="papertitle">Control of Pneumatic Artificial Muscles with SNN-based Cerebellar-like Model</span>
                </a>
                <br>
                <a href="https://cuhkleggedrobotlab.github.io/people/hongbo/" class="black-link">Hongbo Zhang</a>*,
                <a class="myself"><strong>Yunshuang Li</strong>*</a>,
                Yipin Guo*,
                Xinyi Chen,
                <a href="https://ieeexplore.ieee.org/author/38264350700" class="black-link">Qinyuan Ren</a>
                <br>
                <em>International Conference on Social Robotics (ICSR), 2021</em>
                <br>
                <a href="https://arxiv.org/abs/2109.10750#:~:text=Inspired%20by%20Cerebellum%27s%20vital%20functions%20in%20control%20of,controlling%20a%201-DOF%20robot%20arm%20driven%20by%20PAMs.">arXiv</a>
                /
                <a href="data/poster_SNN.pdf">poster</a>
                <p></p>
                <p>
                  Inspired by Cerebellum's vital functions in control of human's physical movement, we propose a neural network model of Cerebellum based on spiking neuron networks (SNNs). We apply the model as a feed-forward controller in controlling a 1-DOF robot arm driven by PAMs. 
                </p>
              </td>
            </tr>	 -->

            

      
      <!--Image -> Video no yellow bg
      <tr onmouseout="db3d_stop()" onmouseover="db3d_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='db3d_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/owl.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/owl.png' width="160">
          </div>
          <script type="text/javascript">
            function db3d_start() {
              document.getElementById('db3d_image').style.opacity = "1";
            }

            function db3d_stop() {
              document.getElementById('db3d_image').style.opacity = "0";
            }
            db3d_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://dreambooth3d.github.io/">
            <span class="papertitle">DreamBooth3D: Subject-Driven Text-to-3D Generation</span>
          </a>
          <br>
          
  <a href="https://amitraj93.github.io/">Amit Raj</a>, <a href="https://www.linkedin.com/in/srinivas-kaza-64223b74">Srinivas Kaza</a>, <a href="https://poolio.github.io/">Ben Poole</a>, <a href="https://m-niemeyer.github.io/">Michael Niemeyer</a>, <a href="https://natanielruiz.github.io/">Nataniel Ruiz</a>, 
  <a href="https://bmild.github.io/">Ben Mildenhall</a>, <a href="https://scholar.google.com/citations?user=I2qheksAAAAJ">Shiran Zada</a>, <a href="https://kfiraberman.github.io/">Kfir Aberman</a>, <a href="http://people.csail.mit.edu/mrub/">Michael Rubinstein</a>, 
          <strong>Jonathan T. Barron</strong>, <a href="http://people.csail.mit.edu/yzli/">Yuanzhen Li</a>, <a href="https://varunjampani.github.io/">Varun Jampani</a>
          <br>
          <em>ICCV</em>, 2023
          <br>
          <a href="https://dreambooth3d.github.io/">project page</a> / 
          <a href="https://arxiv.org/abs/2303.13508">arXiv</a>
          <p></p>
          <p>Combining DreamBooth (personalized text-to-image) and DreamFusion (text-to-3D) yields high-quality, subject-specific 3D assets with text-driven modifications</p>
        </td>
      </tr>-->

          </tbody></table>
          
          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Service</h2>
                <p>
                  <b>Workshop reviewer</b>: NeurIPS, ICRA<br>
                  <b>CIS 5190</b> Applied Machine Learning: TA in Spring 24 at UPenn.<br>
                  <b>CIS 5200</b> Machine Learning: TA in Fall 23 at UPenn.<br>
                  <b><a href="https://github.com/MEAM520" class="black-link">MEAM 5200 Introduction to Robotics</a></b>: TA in Spring 23 at UPenn.<br>
                  <b><a herf="https://fife.cis.upenn.edu" class="black-link">Fife-Penn Python Club</a></b>: Instructor in Spring 23 at G.W. Carver High School.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Adwards</h2>
                <p>
                  <b>SEAS MS Outstanding Research Award</b> 2024<br>
                  <b>President Gutmann Leadership Award</b> administered by GAPSA, UPenn, 2023<br>
                  <b>Best Paper Award</b> at CoRL LEAP workshop, 2023<br>
                  <b>CoRL 2023 Travel Grant</b>, 2023<br>
                  <b>Summer Internship Award</b> issued by GAPSA at Penn, 2023<br>
                  <!-- <b>Chiang Chen Overseas Graduate Fellowship</b> (10 students each year in mainland China, 50k$), 2022<br> -->
                  <b>Outstanding Graduate</b> of Zhejiang Province issued by Department of Education of Zhejiang Province, 2022<br>
                  <b>National Scholarship</b> issued by the Ministry of Education of PRC, 2021<br>
                  <b>First Prize Scholarship</b> issued by Zhejiang University, 2018-2021
                </p>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Media</h2>
                <p>
                  <a herf="https://www.grasp.upenn.edu/news/2024-seas-graduate-awards-ceremony/" class="black-link">The 2024 SEAS Graduate Awards Ceremony Spotlight</a>
                </p>
              </td>
            </tr>
          </tbody></table>
          

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Inspired by the template <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
