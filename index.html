<!DOCTYPE HTML>
<html lang="en">
  <head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-3E7JJ239K6"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-3E7JJ239K6');
    </script>

    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yunshuang Li</title>

    <meta name="author" content="Jon Barron">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="images/head_robo.png" type="image/png">
    <!--<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
  </head>

  <style>
      .black-link {
      color: black; /* Set the desired color for the link */
    }
      .myself {
        color: black;
        text-decoration: underline;
      }
  </style>
  <style>
        body {
            font-family: Arial, sans-serif; /* Default font for the entire body */
        }

        .centered-text {
            text-align: center; /* Center-align text within the container */
            font-family: "Times New Roman", Times, serif; /* Different font for the centered text */
        }
  </style>
  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center; font-size: 32px; font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif; font-weight: 700;">
                  Yunshuang Li
                </p>
                <p>Hello world!
                </p>
                <p>
                  I am a second-year Ph.D. student in Computer Science at the University of Southern California, where I am co-advised by <a href="https://danielseita.github.io/">Prof. Daniel Seita</a> and <a href="https://uscresl.org/principal-investigator/">Prof. Gaurav Sukhatme</a>.
                </p>
                <p>
                  I received an M.S. degree in Robotics at the University of Pennsylvania in May 2024. I was a member of the <a href="https://www.seas.upenn.edu/~dineshj/pennpal/index.html">Perception, Action, & Learning (PAL) Research Group</a>, advised by <a href="https://www.seas.upenn.edu/~dineshj/">Prof. Dinesh Jayaraman</a>. Previously, I worked with <a href="https://www.cse.cuhk.edu.hk/~qdou/index.html">Prof. Qi Dou</a> at the Chinese University of Hong Kong as an undergrad researcher. I received my Honorable B.S. degree in Automatic Control from <a href="http://ckc.zju.edu.cn/ckcen/">Chu Kochen Honors College</a>, Zhejiang University in 2022.
                </p>
                <p style="text-align:center">
                  <a href="mailto:yunshuan@usc.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/Yunshuang_Li_CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=fpx2AWYAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/yunshuang-li-166034293/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/YunshuangL">Twitter</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/Yunshuang_headshot.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Yunshuang_headshot.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <hr style="border: none; border-top: 1px solid #ca0783; margin: 0px 0;">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2 class="name" style="font-size: 24px; font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif; font-weight: 300;">
                  Research</h2>
                <p>
                  I'm interested in learning for dexterous robot manipulation. Specifically, my research explores how to effectively improve robotic capabilities under resource constraints by making effective use of available data and physical resources.
                  My ultimate goal is to advance robotic manipulation toward human-level dexterity, enabling robots to learn and adapt effectively.
                </p>
                <p>
                  (* indicates equal contribution, ‚Ä† indicates equal advising)
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="GD2P_stop()" onmouseover="GD2P_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div style="
                margin-top:1px;
                background-color:#ca0783; 
                color:white; 
                padding:1px 12px; 
                border-radius:7px; 
                text-align:center;
                font-size:14px;">
              <em>In Submission</em>
              </div>
                <div class="one">
                  <div class="two" id='GD2P'>
                    <video  width=100% height=100% muted autoplay loop>
                    <source src="images/GD2P.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                    </video></div>
                  <img src='images/GD2P.png' width="160">
                </div>
                <script type="text/javascript">
                  function GD2P_start(){
                    document.getElementById('GD2P').style.opacity = "1";
                  }
  
                  function GD2P_stop() {
                    document.getElementById('GD2P').style.opacity = "0";
                  }
                  GD2P_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://geodex2p.github.io/">
                  <span class="papertitle">Learning Geometry-Aware Nonprehensile Pushing and Pulling
                    with Dexterous Hands</span>
                </a>
                <br>
                <a class="myself"><strong>Yunshuang Li</strong></a>,
                <a href="https://yiyang0207.github.io/" class="black-link">Yiyang Ling</a>,
                <a href="https://uscresl.org/principal-investigator/" class="black-link">Gaurav Sukhatme</a><sup>&dagger;</sup>,
                <a href="ttps://danielseita.github.io/" class="black-link">Daniel Seita</a><sup>&dagger;</sup>
                <br>
                <em>In Submission</em>
                <br>
                <a href="https://arxiv.org/abs/2509.18455">arXiv</a>
                /
                <a href="https://geodex2p.github.io/">project page</a>
                /
                <a href="https://geodex2p.github.io/static/pdfs/Video.mp4" >video</a>
                /
                <a href="" >code</a>
                <p>We propose Geometry-aware Dexterous Pushing and Pulling, a method for
                  nonprehensile manipulation which frames the problem as
                  synthesizing and learning pre-contact dexterous hand poses that
                  lead to effective pushing and pulling. </p>
              </td>
            </tr>

            <tr onmouseout="hodor_stop()" onmouseover="hodor_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div style="
                margin-top:1px;
                background-color:#ca0783; 
                color:white; 
                padding:1px 12px; 
                border-radius:7px; 
                text-align:center;
                font-size:14px;">
              CoRL
              </div>
                <div class="one">
                  <div class="two" id='hodor'>
                    <video  width=100% height=100% muted autoplay loop>
                    <source src="images/hodor.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                    </video></div>
                  <img src='images/hodor.png' width="160">
                </div>
                <script type="text/javascript">
                  function hodor_start(){
                    document.getElementById('hodor').style.opacity = "1";
                  }
  
                  function hodor_stop() {
                    document.getElementById('hodor').style.opacity = "0";
                  }
                  hodor_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://sites.google.com/view/hodor-corl24">
                  <span class="papertitle">Task-Oriented Hierarchical Object Decomposition for Visuomotor Control</span>
                </a>
                <br>
                Jianing Qian,
                <a class="myself"><strong>Yunshuang Li</strong></a>,
                <a href="https://bucherb.github.io/" class="black-link">Bernadette Bucher</a>,
                <a href="https://www.seas.upenn.edu/~dineshj/" class="black-link">Dinesh Jayaraman</a>
                <br>
                <em>Conference on Robot Learning (CoRL), 2024</em>
                <br>
                <a href="https://arxiv.org/abs/2411.01284">arXiv</a>
                /
                <a href="https://sites.google.com/view/hodor-corl24">project page</a>
                /
                <a href="https://drive.google.com/file/d/1Ixt3LBgxuSdp3g5lp2VNskAuaTH6Vc-Z/view" >video</a>
                /
                <a href= >code</a>
                <p>We propose to train a large combinatorial family of representations organized by scene entities. This hierarchical object decomposition for task-oriented representations permits selectively assembling different representations specific to each task while scaling in representational capacity with the complexity of the scene and the task.</p>
              </td>
            </tr>

            <tr onmouseout="DROID_stop()" onmouseover="DROID_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div style="
                margin-top:1px;
                background-color:#ca0783; 
                color:white; 
                padding:1px 12px; 
                border-radius:7px; 
                text-align:center;
                font-size:14px;">
              RSS
              </div>
                <div class="one">
                  <div class="two" id='DROID'>
                    <video  width=100% height=100% muted autoplay loop>
                    <source src=" " type="video/mp4">
                    Your browser does not support the video tag.
                    </video></div>
                  <img src='images/droid.png' width="160">
                </div>
                <script type="text/javascript">
                  function DROID_start(){
                    document.getElementById('Open').style.opacity = "1";
                  }
  
                  function DROID_stop() {
                    document.getElementById('Open').style.opacity = "0";
                  }
                  DROID_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://droid-dataset.github.io/">
                  <span class="papertitle">DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset</span>
                </a>
                <br>
                DROID Dataset Team
                <br>
                <em> Robotics: Science and Systems (RSS), 2024<b></b></em>
                <br>
                <a href="https://arxiv.org/abs/2403.12945">arXiv</a>
                /
                <a href="https://droid-dataset.github.io/">project page</a>
                /
                <a href="https://droid-dataset.github.io/videos/droid_teaser_animated.mp4">video</a>
                <p></p>
              </td>
            </tr>

            <tr onmouseout="UVD_stop()" onmouseover="UVD_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div style="
                margin-top:1px;
                background-color:#ca0783; 
                color:white; 
                padding:1px 12px; 
                border-radius:7px; 
                text-align:center;
                font-size:14px;">
              ICRA
              </div>
                <div class="one">
                  <div class="two" id='UVD'>
                    <video  width=100% height=100% muted autoplay loop>
                    <source src="images/cloth_camera.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                    </video></div>
                  <img src='images/UVD.png' width="160">
                </div>
                <script type="text/javascript">
                  function UVD_start(){
                    document.getElementById('UVD').style.opacity = "1";
                  }
  
                  function UVD_stop() {
                    document.getElementById('UVD').style.opacity = "0";
                  }
                  UVD_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://zcczhang.github.io/UVD/">
                  <span class="papertitle">Universal Visual Decomposer: Long-Horizon Manipulation Made Easy</span>
                </a>
                <br>
                <a href="https://zcczhang.github.io/" class="black-link">Zichen Zhang</a>*,
                <a class="myself"><strong>Yunshuang Li</strong></a>*,
                <a href="https://obastani.github.io/" class="black-link">Osbert Bastani</a>,
                <a href="https://homes.cs.washington.edu/~abhgupta/" class="black-link">Abhishek Gupta</a>,
                <a href="https://www.seas.upenn.edu/~dineshj/" class="black-link">Dinesh Jayaraman</a>,
                <a href="https://www.seas.upenn.edu/~jasonyma/" class="black-link">Yecheng Jason Ma</a><sup>&dagger;</sup>,
                <a href="https://lucaweihs.github.io" class="black-link">Luca Weihs</a><sup>&dagger;</sup>
                <br>
                <em> <a href="https://leap-workshop.github.io/" class="black-link">LEAP worshop</a> @ CoRL 2023<b style="color: #ca0783;"> (Oral, Best paper award)</b> </em> <br>
                <em> <a href="https://sites.google.com/view/fmdm-neurips23/" class="black-link">FMDM worshop</a> @ NeurIPS 2023 <b style="color: #ca0783;">(Oral, 6/112)</b></em> <br>
                <em> International Conference on Robotics and Automation (ICRA) 2024 <b style="color: #ca0783;">(Best Computer Vision Paper Finalist)</b></em>
                <br>
                <a href="https://arxiv.org/abs/2310.08581">arXiv</a>
                /
                <a href="https://zcczhang.github.io/UVD/">project page</a>
                /
                <a href=https://zcczhang.github.io/UVD/assets/videos/ICRA24_3197_VI_i.mp4>video</a>
                /
                <a href="https://github.com/zcczhang/UVD/">code</a>
                <p></p>
              </td>
            </tr>

            <tr onmouseout="Open_stop()" onmouseover="Open_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div style="
                margin-top:1px;
                background-color:#ca0783; 
                color:white; 
                padding:1px 12px; 
                border-radius:7px; 
                text-align:center;
                font-size:14px;">
              ICRA
              </div>
                <div class="one">
                  <div class="two" id='Open'>
                    <video  width=100% height=100% muted autoplay loop>
                    <source src=" " type="video/mp4">
                    Your browser does not support the video tag.
                    </video></div>
                  <img src='images/open.png' width="160">
                </div>
                <script type="text/javascript">
                  function Open_start(){
                    document.getElementById('Open').style.opacity = "1";
                  }
  
                  function Open_stop() {
                    document.getElementById('Open').style.opacity = "0";
                  }
                  Open_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://robotics-transformer-x.github.io/">
                  <span class="papertitle">Open X-Embodiment: Robotic Learning Datasets and RT-X Models</span>
                </a>
                <br>
                Open X-Embodiment Team
                <br>
                <em> International Conference on Robotics and Automation (ICRA) 2024 <b style="color: #ca0783;">(Best Conference Paper Award)</b></em>
                <br>
                <a href="https://arxiv.org/abs/2310.08864">arXiv</a>
                /
                <a href="https://robotics-transformer-x.github.io/">project page</a>
                /
                <a href="https://robotics-transformer-x.github.io/video/teaser_compressed.mp4">video</a>
                /
                <a href="https://github.com/google-deepmind/open_x_embodiment">code</a>
                <p></p>
              </td>
            </tr>

            <tr onmouseout="im2contact_stop()" onmouseover="im2contact_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div style="
                margin-top:1px;
                background-color:#ca0783; 
                color:white; 
                padding:1px 12px; 
                border-radius:7px; 
                text-align:center;
                font-size:14px;">
              CoRL
              </div>
              <div class="one">
                <div class="two" id='im2contact'>
                  <video  width=100% height=100% muted autoplay loop>
                  <source src="images/im2contact.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                <img src='images/im2contact.png' width="160">
              </div>
              <script type="text/javascript">
                function im2contact_start(){
                  document.getElementById('im2contact').style.opacity = "1";
                }

                function im2contact_stop() {
                  document.getElementById('im2contact').style.opacity = "0";
                }
                im2contact_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=h8halpbqB-&referrer=%5Bthe%20profile%20of%20Dinesh%20Jayaraman%5D(%2Fprofile%3Fid%3D~Dinesh_Jayaraman2)">
                <span class="papertitle">Vision-Based Contact Localization Without Touch or Force Sensing</span>
              </a>
              <br>
              Leon Kim,
              <a class="myself"><strong>Yunshuang Li</strong></a>, 
              <a href="https://www.grasp.upenn.edu/people/michael-posa/" class="black-link">Michael Posa</a>,
              <a href="https://www.seas.upenn.edu/~dineshj/" class="black-link">Dinesh Jayaraman</a>
              <br>
              <em>Conference on Robot Learning (CoRL), 2023</em>
              <br>
              <a href="https://sites.google.com/view/im2contact/home">arXiv</a>
              /
              <a href="https://sites.google.com/view/im2contact/home">project page</a>
              /
              <a href="https://sites.google.com/view/im2contact/home">video</a>
              /
              <a href="https://sites.google.com/view/im2contact/home">code</a>
              <p></p>
              <p>
                We propose a challenging vision-based extrinsic contact localization task: with only a single RGB-D camera view of a robot workspace, identify when and where an object held by the robot contacts the rest of the environment. 
              </p>
            </td>
          </tr>

            <tr onmouseout="MICCAI_stop()" onmouseover="MICCAI_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div style="
                margin-top:1px;
                background-color:#ca0783; 
                color:white; 
                padding:1px 12px; 
                border-radius:7px; 
                text-align:center;
                font-size:14px;">
              CMPB
              </div>
                <div class="one">
                  <div class="two" id='MICCAI_image'>
                    <img src='images/challenge_after.png' width="160"></div>
                  <img src='images/challenge_before.png' width="160">
                </div>
                <script type="text/javascript">
                  function MICCAI_start() {
                    document.getElementById('MICCAI_image').style.opacity = "1";
                  }

                  function MICCAI_stop() {
                    document.getElementById('MICCAI_image').style.opacity = "0";
                  }
                  MICCAI_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0169260723002262">
                  <span class="papertitle">PEg TRAnsfer Workflow recognition challenge report: Do multimodal data improve recognition?</span>
                </a>
                <br>
                Arnaud Huaulm√©, Kanako Harada, (et al., including <a class="myself"><strong>Yunshuang Li</strong></a>, 
                <a href="https://www.researchgate.net/profile/Yonghao-Long-2" class="black-link">Yonghao Long</a>, 
                <a href="https://www.cse.cuhk.edu.hk/~qdou/" class="black-link">Qi Dou</a>)
                <br>
                <em>Computer Methods and Programs in Biomedicine, 2023</em>
                <br>
                <a href="https://arxiv.org/abs/2202.05821">arXiv</a>
                <p></p>
                <p>
                  This is the report paper on Workflow Recognition Challenge in MICCAI 2021. I lead the MedAIR team and rank the first over all the 5 rank method in one sub-challenge on multi-modal (videos and kinematics) workflow recognition of robotic surgery videos.
              </td>
            </tr>		


            <!-- <tr onmouseout="Collaborate_stop()" onmouseover="Collaborate_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='Collaborate_image'>
                    <img src='images/collaborative_1.png' width="160"></div>
                  <img src='images/collaborative_2.png' width="160">
                </div>
                <script type="text/javascript">
                  function Collaborate_start() {
                    document.getElementById('Collaborate_image').style.opacity = "1";
                  }

                  function Collaborate_stop() {
                    document.getElementById('Collaborate_image').style.opacity = "0";
                  }
                  Collaborate_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9517621">
                  <span class="papertitle">Collaborative Recognition of Feasible Region with Aerial and Ground Robots through DPCN</span>
                </a>
                <br>
                <a class="myself"><strong>Yunshuang Li</strong></a>,
                Zheyuan Huang, 
                <a href="https://www.researchgate.net/profile/Zexi-Chen-3" class="black-link">Zexi Chen</a>, 
                <a href="https://ywang-zju.github.io/" class="black-link">Yue Wang</a>, 
                <a href="https://www.researchgate.net/profile/Rong-Xiong" class="black-link">Rong Xiong</a>
                <br>
                <em>IEEE International Conference on Real-time Computing and Robotics (RCAR), 2021</em>
                <br>
                <a href="https://arxiv.org/abs/2103.00947v2">arXiv</a>
                <p></p>
                <p>
                  We present a collaborative system with aerial and ground robots to gain precise recognition of feasible region. Taking the aerial robots' advantages of having large scale variance of view points of the same route which the ground robots is on, the collaboration work provides global information of road segmentation for the ground robot, thus enabling it to obtain feasible region and adjust its pose ahead of time.
                </p>
              </td>
            </tr>		

            <tr onmouseout="SNN_stop()" onmouseover="SNN_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='SNN_image'>
                    <img src='images/SNN_after_large.png' width="160"></div>
                  <img src='images/SNN_before.png' width="160">
                </div>
                <script type="text/javascript">
                  function SNN_start() {
                    document.getElementById('SNN_image').style.opacity = "1";
                  }

                  function SNN_stop() {
                    document.getElementById('SNN_image').style.opacity = "0";
                  }
                  SNN_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://link.springer.com/chapter/10.1007/978-3-030-90525-5_79">
                  <span class="papertitle">Control of Pneumatic Artificial Muscles with SNN-based Cerebellar-like Model</span>
                </a>
                <br>
                <a href="https://cuhkleggedrobotlab.github.io/people/hongbo/" class="black-link">Hongbo Zhang</a>*,
                <a class="myself"><strong>Yunshuang Li</strong>*</a>,
                Yipin Guo*,
                Xinyi Chen,
                <a href="https://ieeexplore.ieee.org/author/38264350700" class="black-link">Qinyuan Ren</a>
                <br>
                <em>International Conference on Social Robotics (ICSR), 2021</em>
                <br>
                <a href="https://arxiv.org/abs/2109.10750#:~:text=Inspired%20by%20Cerebellum%27s%20vital%20functions%20in%20control%20of,controlling%20a%201-DOF%20robot%20arm%20driven%20by%20PAMs.">arXiv</a>
                /
                <a href="data/poster_SNN.pdf">poster</a>
                <p></p>
                <p>
                  Inspired by Cerebellum's vital functions in control of human's physical movement, we propose a neural network model of Cerebellum based on spiking neuron networks (SNNs). We apply the model as a feed-forward controller in controlling a 1-DOF robot arm driven by PAMs. 
                </p>
              </td>
            </tr>	 -->

            

      
      <!--Image -> Video no yellow bg
      <tr onmouseout="db3d_stop()" onmouseover="db3d_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='db3d_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/owl.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/owl.png' width="160">
          </div>
          <script type="text/javascript">
            function db3d_start() {
              document.getElementById('db3d_image').style.opacity = "1";
            }

            function db3d_stop() {
              document.getElementById('db3d_image').style.opacity = "0";
            }
            db3d_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://dreambooth3d.github.io/">
            <span class="papertitle">DreamBooth3D: Subject-Driven Text-to-3D Generation</span>
          </a>
          <br>
          
  <a href="https://amitraj93.github.io/">Amit Raj</a>, <a href="https://www.linkedin.com/in/srinivas-kaza-64223b74">Srinivas Kaza</a>, <a href="https://poolio.github.io/">Ben Poole</a>, <a href="https://m-niemeyer.github.io/">Michael Niemeyer</a>, <a href="https://natanielruiz.github.io/">Nataniel Ruiz</a>, 
  <a href="https://bmild.github.io/">Ben Mildenhall</a>, <a href="https://scholar.google.com/citations?user=I2qheksAAAAJ">Shiran Zada</a>, <a href="https://kfiraberman.github.io/">Kfir Aberman</a>, <a href="http://people.csail.mit.edu/mrub/">Michael Rubinstein</a>, 
          <strong>Jonathan T. Barron</strong>, <a href="http://people.csail.mit.edu/yzli/">Yuanzhen Li</a>, <a href="https://varunjampani.github.io/">Varun Jampani</a>
          <br>
          <em>ICCV</em>, 2023
          <br>
          <a href="https://dreambooth3d.github.io/">project page</a> / 
          <a href="https://arxiv.org/abs/2303.13508">arXiv</a>
          <p></p>
          <p>Combining DreamBooth (personalized text-to-image) and DreamFusion (text-to-3D) yields high-quality, subject-specific 3D assets with text-driven modifications</p>
        </td>
      </tr>-->

          </tbody></table>
          
          <hr style="border: none; border-top: 1px solid #ca0783; margin: 0px 0;">
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2 class="name" style="font-size: 24px; font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif; font-weight: 300;">
                  Service</h2>
                <p>
                  <b>Mentorship</b>: <a herf="https://www.ethanfoong.com/" class="black-link">Ethan Foong</a><br>
                  <b>Conference reviewer</b>: CoRL<br>
                  <b>Workshop reviewer</b>: NeurIPS, ICRA, RSS<br>
                  <b>Teaching Assistant at USC</b>: CSCI 103: Introduction to Programming, Fa25.<br>
                  <b>Teaching Assistant at UPenn</b>: <a href="https://github.com/MEAM520" class="black-link">MEAM 5200: Introduction to Robotics</a>, Sp23; CIS 5200: Machine Learning, Fa23; CIS 5190, Applied Machine Learning, Sp24.<br>
                  <b><a herf="https://fife.cis.upenn.edu" class="black-link">Fife-Penn Python Club</a></b>: Instructor in Spring 23 at G.W. Carver High School.
                </p>
              </td>
            </tr>
          </tbody></table>

          <hr style="border: none; border-top: 1px solid #ca0783; margin: 0px 0;">
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2 class="name" style="font-size: 24px; font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif; font-weight: 300;">
                  Awards</h2>
                <p>
                  <b>----PhD----</b><br>
                  <b>USC Viterbi School of Engineering Fellowship</b>, 2024<br>
                  <b>USC WiSE Graduate Top-Off Award</b>(10k$), 2024<br>
                  <b>CoRL 2024 Travel Grant</b>, 2024<br>
                  <b>----Master----</b><br>
                  <b>Best Conference Paper Award</b> ICRA, 2024<br>
                  <b>UPenn MS Outstanding Research Award</b>, 2024<br>
                  <b>UPenn President Gutmann Leadership Award</b>, 2023<br>
                  <b>UPenn Summer Internship Award</b>, 2023<br>
                  <b>Best Paper Award</b> at CoRL LEAP workshop, 2023<br>
                  <b>CoRL 2023 Travel Grant</b>, 2023<br>
                  <!-- issued by GAPSA at Penn -->
                  <b>----Undergrad----</b><br>
                  <b>Chiang Chen Overseas Graduate Fellowship</b> (10 students each year in mainland China, 50k$), 2022<br>
                  <b>Outstanding Graduate</b> of Zhejiang Province issued by Department of Education of Zhejiang Province, 2022<br>
                  <b>National Scholarship</b> issued by the Ministry of Education of PRC, 2021<br>
                  <b>First Prize Scholarship</b> issued by Zhejiang University, 2018-2021
                </p>
              </td>
            </tr>
          </tbody></table>

          <hr style="border: none; border-top: 1px solid #ca0783; margin: 0px 0;">
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2 class="name" style="font-size: 24px; font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif; font-weight: 300;">
                  Media</h2>
                <p>
                  <a herf="https://www.grasp.upenn.edu/news/2024-seas-graduate-awards-ceremony/" class="black-link">The 2024 SEAS Graduate Awards Ceremony Spotlight</a>
                </p>
              </td>
            </tr>
          </tbody></table>
          

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Inspired by the template <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
    <nav>
      <a herf="CS698_teaching_video.html">CS698 Teaching Video and Write-up</a>
    </nav>
  </body>
</html>
